# XSPEC script to fit a thermal plasma
# THIS SCRIPT REQUIRES XSPEC 12!

set version "\$Id: tbabs_vapec.xcm 3502 2009-08-04 19:22:49Z psb6 $"
set date_value [ exec date ]
puts "[ exec hostname ] $date_value $version"

# ACIS Extract is expected to prepend statments like the following:
# set spectrum_filename       "104357.47-593251.3.pi"
# set extra_spectrum_filename "104357.47-593251.3_grp5.0.pi"
# set ignore_spec             "1-34,549-**"
# set extra_ignore_spec      "1,98"
# set model_name              "nogrp_tbabs_vapec"
# set c_stat_flag             1
# set bkg_ignore_spec         "1-7,686-**"
# set cplinear_energies       {0.5 1.0 1.5 2.0 3.0 4.0 5.0 6.0 7.0 8.0}
# set model_directory         "/users/research/patb/TARA/code/ae/xspec_scripts"
# set interactive_flag        0
# set spectrum_description    "NET_CNTS=150-10.7"

set xs_return_result 1
set tcl_precision 12
autosave off

# IMPORTANT NOTE:
# "OTHER MODEL" will mark sections (separated by blank lines) that
# must be changed if you rewrite this script with your own model

# USER may change boolean command echoing, useful for debugging.
set Echo_Mode 0 
set xs_echo_script $Echo_Mode

# USER may change the chatter level, useful for debugging, ranges from 0 to 25 
set Chatter_Level 10
chatter $Chatter_Level

# To omit execution of "error" commands (which sometimes hang xspec):
#set skip_errors 1
# OR, to compute parameter errors:
set skip_errors 0

# "OTHER MODEL"
# USER may change initial parameters
# Include digits after the decimal to avoid string-type FITS keyword.

# These *_min and *_max parameters are used as "soft parameter limits" in XSPEC, and are used to identify "limit violations" in our table generator. 
set kT0_min   0.1
set kT0       2.0
set kT0_max  15.0

set NH0_min   0.01
set NH0       1.0
set NH0_max 100.0

# These abundances are frozen at the values adopted by the XEST study (Gudel07), relative to (Anders89), scaled to (Wilms00), using the tbabs absorption code in XSPEC.  (See Feb. 2008 email discussion with Marc Gagne.)
set He0  1.0  
set  C0  1.0  
set  N0  1.0  
set  O0  0.75 
set Ne0  1.17 
set Mg0  0.39 
set Al0  1.0  
set Si0  0.57 
set  S0  0.55 
set Ar0  0.78 
set Ca0  0.29 
set Fe0  0.35 
set Ni0  0.32 

# A reasonable initial value for the "norm" parameter helps prevents fits from
# running off into the weeds.
set norm0 1E-6



# USER may change the confidence intervals used in calculating
# errors on fit parameters and errors on fluxes.
# Beware that you must express the desired parameter confidence intervals
# in units of "sigma" and you must express the desired flux
# confidence intervals in units of percent probability.
# For 2-sided confidence intervals on a Gaussian distribution the 
# relationship between these two representations is given in a table
# in the ahelp page for the Sherpa "projection" command, and in 
# Section 2.2 of the manual for XSPEC12.
# That table is shown below for convenience:
#   1.0 -sigma = 68%
#   1.65-sigma = 90%
#   2.0 -sigma = 95.5%
#   2.57-sigma = 99.0%
#   3.0 -sigma = 99.7%

# confidence intervals for fit parameters (in units of sigma)
# Always use a decimal point!
set Conf_Level_Par 1.65

# The square relationship between confidence level sigma (above) and the 
# corresponding change in the fit statistic is described in Section 2.2 of the 
# XSPEC12 manual and in the ahelp page for the Sherpa "projection" command.
set target_stat_rise_for_error [expr $Conf_Level_Par * $Conf_Level_Par]

# Choose an accuracy requirement for the fit statistic (passed to the "fit" and "error" commands)
# that is small compared to $target_stat_rise_for_error.
# NOTE (from the XSPEC manual) that this is a requirement on the absolute difference  
# in the fit statistic between iterations of a search, NOT a fractional change.
set stat_accuracy_for_fit   [expr $target_stat_rise_for_error/100.0 ]
set stat_accuracy_for_error [expr $target_stat_rise_for_error/ 20.0 ]
puts [format "\nTolerance on the statistic is %5.3f for fitting and %5.3f for error estimation.\n" $stat_accuracy_for_fit $stat_accuracy_for_error]

method leven 100 $stat_accuracy_for_fit


# confidence intervals for flux estimates (in units of %)
set Conf_Level_Flux 90

# USER may change EbandLo, EbandHi, BandName and Number_Runs
# for calculating fluxes and their errors over interested
# energy bands. 
set EbandLo   {0.5  2   0.5}
set EbandHi   {2    8   8  }
#set BandName {H2  28  H8  }
set BandName  {0P5_2  2_8  0P5_8  }
set Number_Runs 100


# set some other vars
set model_before_errors_savfl "model_before_errors.xcm"
set model_savfl               "model.xcm"
set model_fitsfl              "model.fits"
set temp_headfl               "z_head.txt"
setplot energy
setplot add
query yes


# AE CUSTOMIZATIONS are inserted here!!


# Load the "object" (obj) spectrum as spectrum #1 in data group #1.
data 1:1 $spectrum_filename

# Ignore some channels.
ignore 1:$ignore_spec
ignore bad

# Save the names of the background, RMF, and ARF.
set bkgfile [string trim [tcloutr backgrnd 1]]
if { $bkgfile eq "" } { set bkgfile "none" }  

set respfile [string trim [tcloutr response 1]]
if { $respfile eq "" } { set respfile "none" }  

set arffile [string trim [tcloutr arf 1]]
if { $arffile eq "" } { set arffile "none" }  


if {$c_stat_flag} {
  statistic cstat
  
  # We will eventually be working with two "XSPEC sources".
  # Source #1 will be our astrophysical model, tbabs(vapec), passed through the ACIS response.
  # Source #2 will be our observed background model, cplinear, passed through a flat response.
  # Our target spectrum will be modeled by Source #1 + Source #2, and
  # our background spectrum will be modeled by only Source #2.
   
  # Load background (bkg) spectrum as spectrum #2 in data group #2.
  data 2:2 $bkgfile
  
  # Ignore some channels.
  ignore 2:$bkg_ignore_spec
  ignore bad

  # Make sure XSPEC's background algorithm is disabled. 
  backgrnd 1 none
  backgrnd 2 none
  
  # If we have no background data to fit, then unload the background spectrum.
  # Otherwise, build a cplinear model.
  scan [tcloutr rate 2] "%f" bkg_rate
  
  if {$bkg_rate == 0} {
    data 2:2 none
  } else {
    
    # Detach both the source spectrum (spectrum #1) and the background spectrum (spectrum #2) from the stellar model (source #1).
    response 1:1 none
    response 1:2 none
  
    # Create a "source #2", which is going to be our cplinear background model.
    # Tie it to the background spectrum (#2) through an RMF.
    # We're modeling the observed spectrum, not an astrophysical one, so no ARF is used.  
    response 2:2 $respfile
      
    #Run diagrsp here?
  
    # Load and create a continuous piecewise-linear background model.
    # We used "load" instead of "lmod" because the latter requires that the 
    # user have write permission to the model directory.
    set platform "[exec uname -s]_[exec uname -p]"
    switch $platform {
      Darwin_i386    {load $model_directory/Darwin_i386/libacis_extract.dylib}
      Darwin_powerpc {load $model_directory/Darwin_powerpc/libacis_extract.dylib}
      Linux_unknown  {load $model_directory/Linux_unknown/libacis_extract.so}
      SunOS_sparc    {load $model_directory/SunOS_sparc/libacis_extract.so}
      default {puts "\nERROR: you must add a case to the 'switch' statement in this fitting script corresponding to your platform '$platform', compile the cplinear model for your platform, and place the resulting shared library in $model_directory/$platform/."; tclexit 97}
    }
    
    model 2:bkg cplinear & /*
  
    set max_vertices 10
    
    # The "norm" parameter is assumed to be the last one.
    set bkg_norm_parnum [tcloutr modpar bkg]
  
    # Initialize the parameters of the cplinear model.
    set vertex_is_unused {}
    for {set vertex_number 1} {$vertex_number <= $max_vertices} {incr vertex_number} {
      set this_energy [lindex $cplinear_energies [expr $vertex_number-1]]
      
      # Load the vertex energies AE has passed.
      newpar bkg:$vertex_number $this_energy
      
      # Flag the unused vertexes so that we can keep them frozen forever.
      lappend vertex_is_unused [expr ($this_energy < 0)]
    }
  
    
    # In order to get reasonable starting parameter values for our cplinear model
    # later when we simultaneously fit source and background spectra, 
    # we need to fit the background spectrum alone.
    # We set "delta" component of the free parameters to be small compared to the parameter values.
    xset delta 0.01
    query yes
    
    # Note that the rate* and "norm" parameters are degenerate.  
    # For this initial fit we then find a rough normalization so that the rate* parameters
    # will tend towards values near 1 (for human readability) during the fit.
    # Then we freeze the normalization, thaw the active rate* parameters, and run a fit to get a rough starting point for the model.
    renorm
    freeze bkg:$bkg_norm_parnum
  
    for {set vertex_number 1} {$vertex_number <= $max_vertices} {incr vertex_number} {
      # Skip unused vertices
      if {[lindex $vertex_is_unused [expr $vertex_number-1]]} {continue}
      
      set rate_parnum [expr $vertex_number+$max_vertices]
      thaw   bkg:$rate_parnum
    }
    
    chatter 9
    for {set i 1} {$i <= 1} {incr i} { 
      fit 
    }
    chatter $Chatter_Level 
  
    # We assume that XSPEC's fitting engine would like the "norm" parameter free during
    # the main fit below that uses both target and background data.
    # Thus, we will thaw "norm" and freeze the largest of the "rate" parameters.
    set largest_rate_parnum 1
    set largest_rate_parval 0
    for {set vertex_number 1} {$vertex_number <= $max_vertices} {incr vertex_number} {
      # Skip unused vertices
      if {[lindex $vertex_is_unused [expr $vertex_number-1]]} {continue}
      
      # Grab the value of the "rate" parameter for this vertex.
      set rate_parnum [expr $vertex_number+$max_vertices]
      
      scan [tcloutr param bkg:$rate_parnum] "%f" par_value
      
      if {$par_value > $largest_rate_parval} {
        set largest_rate_parnum $rate_parnum
        set largest_rate_parval $par_value
      }
    }
    freeze bkg:$largest_rate_parnum
    thaw   bkg:$bkg_norm_parnum
   
    
    # And we can establish a connection between the target spectrum (#1) and the
    # cplinear background model (Source #2), again WITHOUT an ARF.
    response 2:1 $respfile
    show data
    
    # We could try to run diagrsp here if we wanted the bkg model to have an identity
    # response instead of the ACIS response, but it probably doesn't matter.  
   
    # Finally, we have to link the parameters between the two instances of cplinear.
    # Discussions with Craig Gordon did not reveal any other way to do this in v12.3.1.
    # The auto-linkage of parameters in two cplinear models occurs only if there are 
    # two spectra with responses linked to the model when it is constructed.  
    # But that is not useful in this context because we need at some point to have
    # only the bkg data linked to the bkg model, so a good initial fit can be done.
    newpar bkg:32 = bkg:11
    newpar bkg:33 = bkg:12
    newpar bkg:34 = bkg:13
    newpar bkg:35 = bkg:14
    newpar bkg:36 = bkg:15
    newpar bkg:37 = bkg:16
    newpar bkg:38 = bkg:17
    newpar bkg:39 = bkg:18
    newpar bkg:40 = bkg:19
    newpar bkg:41 = bkg:20
    newpar bkg:[expr 2*$bkg_norm_parnum] = bkg:$bkg_norm_parnum
  
    # Finally, reattach the source spectrum (spectrum #1) to the stellar model (source #1).
    response 1:1 $respfile
    arf      1:1 $arffile
  }

} else {
  # USER may change the weighting of chi statistics as
  # an alternative approach when there are few counts per group.
  # Available methods are:
  # "standard", "gehrels", "churazov", "model" 
  #
  # Note: if you change weighting method then do not forget to change
  # the name of this xspec script to make it unique,
  # then later you can compare spectral fit results
  # of different statistics and/or different weight methods.
  statistic  chi
  set Weighting_Method "standard"
  weight $Weighting_Method
}


# OPTIONAL
# Load a FROZEN model for an additional background component that is not already modeled or subtracted.
# First, we create an additional XSPEC "source" (#3), connected to our "object" spectrum (#1) through its RMF and ARF.
#response 3:1 $respfile
#arf      3:1 $arffile
#
# Then, we load a FROZEN saved model to fill this "source".
# The xcm file we load here must define a named model to fill "source" #3, e.g.
#    model 3:sky ....
#catch {@../composite_bkgd_model} error_message
#if {$error_message != ""} {
#  puts "ERROR: $error_message"
#  tclexit 98
#}



# Create a model for the astrophysical "object", attached to "XSPEC source #1".
#
# It's convenient to build the object model here, AFTER the bkg model is sorted out,
# for several reasons:
# * It eliminates some "response" and "arf" commands, clarifying the scipt..
# * If we set up a 2-component stellar model with linked abundances and then
#   have to issue "response none" to deal with the bkg model then the linked
#   parameters become unlinked.
#
# We use the observer's parameter ranges as "soft" limits in the model, 
# leaving the "hard" limits at their wider default values.  
# This allows poorly constrained fits to exceed the soft limits; we can
# detect that during analysis of the fit results and identify such fits
# as bad.
#
abund wilm
model 1:obj tbabs(vapec) & /* 

set param_number(NH)        1
set param_number(kT)        2
set param_number(He)        3
set param_number(C)         4
set param_number(N)         5
set param_number(O)         6
set param_number(Ne)        7
set param_number(Mg)        8
set param_number(Al)        9
set param_number(Si)       10
set param_number(S)        11
set param_number(Ar)       12
set param_number(Ca)       13
set param_number(Fe)       14
set param_number(Ni)       15
set param_number(norm)     17

# Assign initial value to each parameter.
foreach param_name [array names param_number] {
  eval newpar obj:$param_number($param_name) $${param_name}0
}

# 2008  The default hard upper limit on kT in the vapec model is larger than the model
# will accept, so below we explicitly set it to 60 keV.
newpar obj:$param_number(NH)  $NH0  ,,, $NH0_min  $NH0_max 
newpar obj:$param_number(kT)  $kT0  ,,, $kT0_min  $kT0_max 60.0

# Establish hard limits on the abundances.
newpar obj:$param_number(He) ,,, 0.1 0.1 3.0 3.0
newpar obj:$param_number(C)  ,,, 0.1 0.1 3.0 3.0
newpar obj:$param_number(N)  ,,, 0.1 0.1 3.0 3.0
newpar obj:$param_number(O)  ,,, 0.1 0.1 3.0 3.0
newpar obj:$param_number(Ne) ,,, 0.1 0.1 3.0 3.0
newpar obj:$param_number(Mg) ,,, 0.1 0.1 3.0 3.0
newpar obj:$param_number(Al) ,,, 0.1 0.1 3.0 3.0
newpar obj:$param_number(Si) ,,, 0.1 0.1 3.0 3.0
newpar obj:$param_number(S)  ,,, 0.1 0.1 3.0 3.0
newpar obj:$param_number(Ar) ,,, 0.1 0.1 3.0 3.0
newpar obj:$param_number(Ca) ,,, 0.1 0.1 3.0 3.0
newpar obj:$param_number(Fe) ,,, 0.1 0.1 3.0 3.0
newpar obj:$param_number(Ni) ,,, 0.1 0.1 3.0 3.0


if {[tcloutr datasets] == 2} {
  # If we simply launch into the "fit" command now then XSPEC will start by running the "renorm" command.
  # This will destroy the nice initial value for the "norm" parameter in the bkg model that we just computed above.
  # So, we briefly freeze the bkg model, run "renorm" to give the stellar model a good start, and then thaw the bkg model.
  freeze bkg:$bkg_norm_parnum
  renorm
  thaw   bkg:$bkg_norm_parnum
}  



# If observer specifies a null range for a parameter then freeze it.
if {$NH0_min  >= $NH0_max}  {eval freeze obj:$param_number(NH)}
if {$kT0_min  >= $kT0_max}  {eval freeze obj:$param_number(kT)}

# get & check D.O.F.
scan [tcloutr dof] "%d" dof_value
if {$dof_value < 1} {
  puts "DOF < 1; fit aborted"
  tclexit 98
} 


# We set "delta" component of the free parameters to be small compared to the parameter values.
xset delta 0.01

# Show model prior to fitting.
show model

# Interact with observer if desired.
if {$interactive_flag} {
  chatter 10
  # Perform a fit and determine an appropriate scaling for Y-axis.
  fit
  cpd /xw

  set ymax [lindex [lsort -real -decreasing [tcloutr plot ldata y]] 0]
  set rescale_command_index [setplot command Rescale Y [expr $ymax/1000.0] [expr $ymax*2] ]
  
  plot ldata resid
  puts "\nInitial fit is shown above.\nFit by hand, then type 'c' to continue with script ...\n"
  interact
  
} else {
  # Fit multiple times to be sure ...
  for {set i 1} {$i <= 2} {incr i} { 
      fit 
  }
  
  # Show the the best fit parameters in the log so we can see how
  # the fit wandered during execution the error commands.
  chatter 10
  show par
  show fit
  chatter $Chatter_Level 
}; # Not interactive


# Create a save file for the model before error estimation.
if {[file exists $model_before_errors_savfl]} { file delete $model_before_errors_savfl }
save model $model_before_errors_savfl



puts "\n\nStep 3:  ERROR CALCULATIONS\n"
# "OTHER MODEL"
if {! $skip_errors} {
  # "OTHER MODEL"
  # calculate parameter errors
  # We seem to recall that a single error command for all three pars led to some
  # trouble, but we can't recall the details.. 
  # Let's gamble that this is now fixed so we can avoid writing a loop here.
  #
  set error_estimation_command {error stopat 10 $stat_accuracy_for_error maximum 4.0 $target_stat_rise_for_error obj:$param_number(NH) obj:$param_number(kT) obj:$param_number(norm)}
  
  if {$interactive_flag} {
    # The observer is responsible for running the "error" command.

    # You really want "query on" mode when "error" is executed interactively so that the observer
    # will notice when a new best fit is found.
    query on
    puts "\nYou must execute the following command to estimate parameter errors:"
    puts "   $error_estimation_command"
    puts "\nIf the fit wants to run off into the weeds, and you prefer to skip the error computation, then execute this:"
    puts "    set skip_errors 1 \n"
    puts "\nWhen you're finished with the error estimation, then type 'c' to continue with script ...\n"
    interact
    query yes
  } else {
    # This script is going to try to run the "error" command.
    
    # Save parameters to variables..
    foreach param_name [array names param_number] {
      scan [eval tcloutr param obj:$param_number($param_name)] "%f" $param_name  
    }
    
    # If any of the parameters are out of range, then do not attempt error estimation.
    if { $NH  < $NH0_min  || $NH  > $NH0_max  || \
         $kT  < $kT0_min  || $kT  > $kT0_max     } {
      puts "Parameter limits violated after fit; skipping error estimation."
      set skip_errors 1
    } else {
      # You really want "query yes" mode when "error" is executed via script so that it will
      # restart the search for errors after new best fit is found.
      query yes
      puts "Running error command:\n   $error_estimation_command"
      if [catch $error_estimation_command exception_msg] {
        # exception caught
        puts "Error command failed to execute:\n  >> $exception_msg \nReverting to original fit ..."
        set skip_errors 1
      } else {
        # Error calculation sucessful
        
        # Save parameters to variables..
        foreach param_name [array names param_number] {
          scan [eval tcloutr param obj:$param_number($param_name)] "%f" $param_name  
        }
    
        # If any of the parameters are out of range, then back out of the error estimation
        # since there is a good chance the parameters were fine after the original fit.
        if { $NH  < $NH0_min  || $NH  > $NH0_max  || \
             $kT  < $kT0_min  || $kT  > $kT0_max     } {
            puts "Parameter limits violated after error search; reverting to original fit."
            set skip_errors 1
          }
      }; # Error calculation sucessful
    }; # Error calculation attempted
  }; # Not interactive
  
  
  if {$skip_errors} {
    # If we're abandoning the error calculation then revert to the saved model.
    puts "Loading model from $model_before_errors_savfl ..."
    eval @$model_before_errors_savfl

    # A "fit" command is required here so that a valid fit exists to support lots of
    # other work below, e.g. flux commands.
    # And I think loading the save file might turn off the "query yes" we had set up.
    query yes
    xset delta 0.01
    fit
  };

  chatter 10
  show par                                 
  show fit
  chatter $Chatter_Level   
};  # calculate parameter errors

if {$interactive_flag && [info exists rescale_command_index]} {
  # Remove the Y-axis scaling command so that we can perform other plotting later.
  setplot delete $rescale_command_index
}  
chatter $Chatter_Level 

  
# Retrieve parameter errors, flags, and statistics
if {$skip_errors} {
  set NH_err_stat    'skipped'
  set kT_err_stat    'skipped'
  set norm_err_stat  'skipped'
  set NH_err_high    0.0
  set kT_err_high    0.0
  set norm_err_high  0.0
  set NH_err_low     0.0
  set kT_err_low     0.0
  set norm_err_low   0.0
} else {
  scan [tcloutr error obj:$param_number(NH)]    "%f %f %s" NH_err_low    NH_err_high    NH_err_stat
  scan [tcloutr error obj:$param_number(kT)]    "%f %f %s" kT_err_low    kT_err_high    kT_err_stat
  scan [tcloutr error obj:$param_number(norm)]  "%f %f %s" norm_err_low  norm_err_high  norm_err_stat
}; #Error calculation sucessful


# Save parameters to variables..
foreach param_name [array names param_number] {
  scan [eval tcloutr param obj:$param_number($param_name)] "%f" $param_name  
}

# get D.O.F. again since observer may have changed it.
scan [tcloutr dof] "%d" dof_value


# create a complete save file for the final model
if {[file exists $model_savfl]} { file delete $model_savfl }
save all $model_savfl



if {$c_stat_flag} {
  set       ps_filename "icounts.ps"
  set extra_ps_filename "ldata.ps"
  scan [tcloutr stat] "%f" cstat_result
  set stat_print [format "Cstat=%.0f" $cstat_result]
  setplot command LAbel  Y  normalized integrated counts
  plot icounts residual

  # We think the cumulative spectrum is a more informative plot for weak
  # sources than the "setplot rebin" approach.  The old commands
  # remain below for reference:

  # see help info for rebin
  # We'll set Max_Nbins to just over half the typical number of "noticed" channels
  # (514) so that in the dim case we get two ~even sized bins.
  # And I guess we'll show Gehrels errors (option "poiss-1").
  # We can't tune Min_Signif to get a specific number of counts per bin
  # because the error term in Min_Signif includes background errors.
# set Max_Nbins 260
# set Min_Signif 2.0
# setplot rebin $Min_Signif $Max_Nbins -1 poiss-1
# plot ldata residual
} else {
  set       ps_filename "ldata.ps"
  set extra_ps_filename "icounts.ps"
  scan [tcloutr stat] "%f" chi_squared
  set red_chi_squared [expr $chi_squared/$dof_value]
  set stat_print [format "\\gx\\d\\gn\\u\\u2\\d=%.2f" $red_chi_squared]
  setplot command LAbel  Y  Counts sec\\\u-1 \\\d keV\\\u-1\\\d
  
  # Find the largest Y-value to be plotted.
  set ymax [lindex [lsort -real -decreasing [tcloutr plot ldata y]] 0]
  plot ldata resid
}

# "OTHER MODEL"
# make a postscript image of spectra 
scan [eval tcloutr param obj:$param_number(NH)] "%f %f" par_value par_delta 
set is_frozen [expr $par_delta < 0]                                                                        
if $is_frozen {
  set NH_print [format "N\\dH\\u={%.3g}" $NH]
} else {
  set NH_print [format "N\\dH\\u=%.3g" $NH]
}

scan [eval tcloutr param obj:$param_number(kT)] "%f %f" par_value par_delta 
set is_frozen [expr $par_delta < 0]                                                                        
if $is_frozen {
  set kT_print [format "kT={%.3g}" $kT]
} else {
  set kT_print [format "kT=%.3g" $kT]
}

if {! [info exists spectrum_description]} {set spectrum_description ""}

setplot command FONT Roman
setplot command TIME OFF
setplot command WIN      1
setplot command viewport 0.12 0.1 0.99 0.9
setplot command LAbel Top $NH_print $kT_print  ${stat_print} ${spectrum_description}
setplot command LAbel File [file tail [tcloutr filename 1]]
#setplot command Rescale Y1 7.E-6 4.E-2

setplot command WIN      2
setplot command viewport 0.12 0.1 0.99 0.8
#setplot command Yaxis  3
#if {$c_stat_flag == 0} {
#  setplot command Rescale    Y2 -3 3
#}
setplot command LAbel  X  Energy (keV)

setplot command WIN  ALL
# A CSize of 1.5 causes the title to be cut off.
# Shrinking the viewport in the X direction is needed to prevent the Y axis
# label from being cut off.
setplot command CSize 1.4
setplot command Rescale x 0.5 8
setplot command LWidth 3

if [info exists ymax] {
  # Force an appropriate Y-axis range
  set rescale_command_index [setplot command Rescale Y [expr $ymax/1000.0] [expr $ymax*2] ]
}

plot
cpd $ps_filename/cps
plot
cpd none
cpd /NULL
if [info exists rescale_command_index] {
  # Remove the Y-axis scaling command so that we can perform other plotting later.
  setplot delete $rescale_command_index
}

# output info to xspec.log
# log none does not actually close the pipe
# so use copy
log z_temp_xspec.log
show all
log none
file copy -force z_temp_xspec.log xspec.log
file delete z_temp_xspec.log

# Results are saved to both an ASCII log file and an ASCII template file that is used to build a FITS header.
set log_id [open xspec.log a]
set HDU_id [open $temp_headfl w]

puts $log_id " "
puts $HDU_id "XTENSION=BINTABLE   /binary table extension"
puts $HDU_id "EXTNAME=$model_name /name of this binary table"
puts $HDU_id "DATE='$date_value'  / Date of model"
puts $HDU_id "VERSION='$version'  / Script version"
  
set units "\[ergs cm-2 s-1\]"

# Calculate interesting fluxes and errors
# First, the observed fluxes for all object model components.
for {set ii 0} {$ii <= [expr [llength $EbandLo]-1]} {incr ii} {
  # 2008 July
  # For some sources one of the cplinear parameters peggs at zero, which causes XSPEC's covariance matrix to have fewer rows than there are free parameters, which causes the "flux" command to fail if you request the flux error calculation.  See email to XSPEC team on July 29, 2008.  There is no simple way to detect this problem, so we have decided to just eliminate the flux error calculation entirely.
 #flux [lindex $EbandLo $ii] [lindex $EbandHi $ii]   err $Number_Runs $Conf_Level_Flux

  flux [lindex $EbandLo $ii] [lindex $EbandHi $ii] noerr 

  # Retrieve the flux information for the object (spectrum #1).
  # The first set of six values are for "source" #1, the object model.
  # The second set of six values are for "source" #2, the cplinear background model (if used).
  scan [tcloutr flux 1] "%f %f %f" flux flux_low flux_high

  set BandDescription "[lindex $EbandLo $ii]:[lindex $EbandHi $ii] keV"
  puts $log_id "Flux ($BandDescription): $flux ${Conf_Level_Flux}% conf_range: ($flux_low : $flux_high)"
  
  puts $HDU_id "F[lindex $BandName $ii] =$flux      / $units Flux ($BandDescription)"
 #puts $HDU_id "F[lindex $BandName $ii]U=$flux_high / $units Flux ($BandDescription) ${Conf_Level_Flux}% 2-sided limit"
 #puts $HDU_id "F[lindex $BandName $ii]L=$flux_low  / $units Flux ($BandDescription) ${Conf_Level_Flux}% 2-sided limit"
} 
#endfor


# Then, the NH-corrected fluxes for all object model components.
newpar obj:$param_number(NH) 0. ,, 0. 
for {set ii 0} {$ii <= [expr [llength $EbandLo]-1]} {incr ii} {
  flux [lindex $EbandLo $ii] [lindex $EbandHi $ii] noerr 

  # Retrieve the flux information for the object (spectrum #1).
  # The first set of six values are for "source" #1, the object model.
  # The second set of six values are for "source" #2, the cplinear background model (if used).
  scan [tcloutr flux 1] "%f %f %f" flux flux_low flux_high

  set BandDescription "[lindex $EbandLo $ii]:[lindex $EbandHi $ii] keV"
  puts $log_id "Flux (NH=0,$BandDescription): $flux ${Conf_Level_Flux}% conf_range: ($flux_low : $flux_high)"
  
  puts $HDU_id "Fc[lindex $BandName $ii] =$flux      / $units Flux ($BandDescription,NH=0)"
 #puts $HDU_id "Fc[lindex $BandName $ii]U=$flux_high / $units Flux ($BandDescription,NH=0) ${Conf_Level_Flux}% 2-sided limit"
 #puts $HDU_id "Fc[lindex $BandName $ii]L=$flux_low  / $units Flux ($BandDescription,NH=0) ${Conf_Level_Flux}% 2-sided limit"
} 
#endfor
newpar obj:$param_number(NH) $NH


puts $log_id "$Conf_Level_Par sigma conf_ranges for NH|kT:($NH_err_low - $NH_err_high)|($kT_err_low - $kT_err_high)"
close $log_id

# "OTHER MODEL"
# more output for FITS file
scan [tcloutr noticed 1] "%d-%d" chnl_low chnl_high

  puts $HDU_id "NH0_MIN=$NH0_min    / \[1.e22 cm-2\] Minimum value for column density"
  puts $HDU_id "NH0=$NH0            / \[1.e22 cm-2\] Initial value of column density"
  puts $HDU_id "NH0_MAX=$NH0_max    / \[1.e22 cm-2\] Maximum value for column density"
  puts $HDU_id "kT0_MIN=$kT0_min    / \[keV\] Minimum value for temperature"
  puts $HDU_id "kT0=$kT0            / \[keV\] Initial value of temperature"
  puts $HDU_id "kT0_MAX=$kT0_max    / \[keV\] Maximum value for temperature"
  
  puts $HDU_id "CHNL_LOW=$chnl_low  / Low channel used"
  puts $HDU_id "CHNL_HI=$chnl_high  / High channel used"
  puts $HDU_id "DOF=$dof_value      / Degree of freedom"
  if {$c_stat_flag} {
    puts $HDU_id "CSTAT=$cstat_result / C-statistic"
  } else {
    puts $HDU_id "CHI_SQR=$red_chi_squared / Reduced chi-squared"
  } 
  puts $HDU_id "NH       =$NH             / \[1.e22 cm-2\] Derived column density"
  puts $HDU_id "NH_ERRU  =$NH_err_high    / \[1.e22 cm-2\] Column $Conf_Level_Par sigma upper limit"
  puts $HDU_id "NH_ERRL  =$NH_err_low     / \[1.e22 cm-2\] Column $Conf_Level_Par sigma lower limit"
  puts $HDU_id "NH_ERRST =$NH_err_stat    / Flag indicating problems with error calculation"
  puts $HDU_id "kT       =$kT             / \[keV\] Derived temperature"
  puts $HDU_id "kT_ERRU  =$kT_err_high    / \[keV\] Temperature $Conf_Level_Par sigma upper limit"
  puts $HDU_id "kT_ERRL  =$kT_err_low     / \[keV\] Temperature $Conf_Level_Par sigma lower limit"
  puts $HDU_id "kT_ERRST =$kT_err_stat    / Flag indicating problems with error calculation"
  puts $HDU_id "NORM     =$norm           / Derived normalization"
  puts $HDU_id "NORMERRU =$norm_err_high  / Normalization $Conf_Level_Par sigma upper limit"
  puts $HDU_id "NORMERRL =$norm_err_low   / Normalization $Conf_Level_Par sigma lower limit"
  puts $HDU_id "NORMERRS =$norm_err_stat  / Flag indicating problems with error calculation"

  puts $HDU_id "He=$He / abundance"
  puts $HDU_id " C= $C / abundance"
  puts $HDU_id " N= $N / abundance"
  puts $HDU_id " O= $O / abundance"
  puts $HDU_id "Ne=$Ne / abundance"
  puts $HDU_id "Mg=$Mg / abundance"
  puts $HDU_id "Al=$Al / abundance"
  puts $HDU_id "Si=$Si / abundance"             
  puts $HDU_id " S= $S / abundance"
  puts $HDU_id "Ar=$Ar / abundance"
  puts $HDU_id "Ca=$Ca / abundance"
  puts $HDU_id "Fe=$Fe / abundance"
  puts $HDU_id "Ni=$Ni / abundance"
 
  # Record frozen state of parameters.
  foreach param_name [array names param_number] {
    scan [eval tcloutr param obj:$param_number($param_name)] "%f %f" par_value par_delta 
    
    if {$par_delta < 0} {
      set is_frozen "T"
    } else {
      set is_frozen "F"
    }

    eval puts $HDU_id \"${param_name}_fz=$is_frozen / parameter is frozen\"
  }

close $HDU_id

# write model to a FITS file
file delete $model_fitsfl   
exec ftemplate template=$temp_headfl outfile=$model_fitsfl
file delete $temp_headfl

# Create a contour plot depicting joint uncertainties between two parameters.
# Someday ....


# If the fit was Cstat make a second plot showing a grouped spectrum.
# If the fit was with grouped data make a second plot showing the ungrouped spectrum.
# We can NOT move this plot code earlier in the script because the "data" command below
# destroys elements of the current fit (covariance matrix) that are used by 
# the flux error calculations above.
if [info exists extra_spectrum_filename] {
                
  # Load the extra spectrum, configure for the specified statistic, and make the appropriate plot.
  data   1:1 $extra_spectrum_filename
  ignore 1:$extra_ignore_spec
  ignore bad

  if {$c_stat_flag} {
    statistic chi; weight gehrels; 
    setplot command LAbel File [file tail [tcloutr filename 1]]
    setplot command LAbel  Y  Counts sec\\\u-1 \\\d keV\\\u-1\\\d
  
    # Find the largest Y-value to be plotted, and force an appropriate Y-axis range.
    set ymax [lindex [lsort -real -decreasing [tcloutr plot ldata y]] 0]
    set rescale_command_index [setplot command Rescale Y [expr $ymax/1000.0] [expr $ymax*2] ]

    cpd $extra_ps_filename/cps
    plot ldata residual
  } else {
    statistic cstat
    setplot command LAbel  Y  normalized integrated counts
    cpd $extra_ps_filename/cps
    plot icounts residual
  }

cpd none
}
exit

